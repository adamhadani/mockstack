{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs + mockstack\n",
    "\n",
    "A few simple examples for using `mockstack` to mock various components in typical LLM-driven use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain dependencies with the following or using `uv` depending on your venv setup:\n",
    "\n",
    "#!pip install -q langchain langchain-openai\n",
    "# or:\n",
    "#!uv pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #1: Static template-based mocking (`filefixtures` strategy)\n",
    "\n",
    "Here we simply use the **filefixtures** strategy to route requesets coming in for a certain URL to a template file with the appropriate name.\n",
    "\n",
    "For the below example you'll want to make sure:\n",
    "\n",
    "- mockstack is running at http://localhost:8000 which are the default settings\n",
    "- `MOCKSTACK__TEMPLATES_DIR` is pointing to a valid directory with a template file called `openai-v1-chat-completions.j2`. See the [included file](./templates/openai-v1-chat-completions.j2) with same name in the ./templates sub-directory of this example for a a template with a valid response based on [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create).\n",
    "- the `MOCKSTACK__FILEFIXTURES_ENABLE_TEMPLATES_FOR_POST` flag is set to true (which it should be by default)\n",
    "\n",
    "\n",
    "If everything is setup correctly, you should get back the mocked response in the correct format from the template and the below assert should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "assert ai_msg.content == \"Hello! How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an identical example, but using OpenAI via Microsoft Azure.\n",
    "\n",
    "The only difference is that under the hood `LangChain` will hit an API URL of the form `/openai/v1/deployments/gpt-4o/chat/completions?api-version=<api-version>`.\n",
    "\n",
    "We can again handle this easily using templates by just creating a file with the name `openai-v1-deployments-gpt-4o-chat-completions.j2`.\n",
    "\n",
    "Remember you can also create conditional logic inside that file using Jinja syntax to return different mock responses depending on the request parameters, for instance using the query parameter `api-version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "assert ai_msg.content == \"Hello! How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #2: Dynamic template-based mocking with ollama integration\n",
    "\n",
    "In this example we use the [ollama-python](https://github.com/ollama/ollama-python) integration to return mock responses that actually come from a real LLM running on your host! This is a great way to go a step further in development, debugging, and integration testing scenarios where LLMs and their non-determinism are important to capture.\n",
    "\n",
    "For this example you will need to make sure you have the following:\n",
    "\n",
    "* mockstack installed with the optional `llm` dependencies:\n",
    "\n",
    "    ```bash\n",
    "    uv pip install mockstack[llm]\n",
    "    ```\n",
    "\n",
    "* [ollama](https://ollama.com/) installed locally with the \"llama3.2\" model (which is typically the default model installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockStack est vraiment amusant. Mais les LLMs sont beaucoup plus amusants.\n",
      "\n",
      "(Note: I translated \"pretty cool\" as \"très amusant\", which is a more formal and common expression in French. If you want to use a more casual tone, I can try \"très sympa\" or \"c'est sympa\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/ollama/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"mockstack is pretty cool. But LLMs are way cooler\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "# Output would not be deterministic, but should be a valid French translation.\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #3: Interruptible Streaming Chain\n",
    "\n",
    "This example shows a more complited workflow you may wish to develop and have some mock responses and/or ollama-based responses for.\n",
    "\n",
    "Here we have an \"Interruptible\", Streaming chain - we engage with an LLM in a multi-step chain, potentially feeding back some of the output to the LLM, while polling an external resource to check if we should \"abort\", in which case we stop streaming, which means the LLM stops charging per token at that moment in time (give or take a couple of tokens per OpenAI and friends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** STREAM **\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No generation chunks were returned",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      3\u001b[39m llm = ChatOpenAI(\n\u001b[32m      4\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     temperature=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     api_key=\u001b[33m\"\u001b[39m\u001b[33mSOME_STRING_THAT_DOES_NOT_MATTER\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrite me a song about sparkling water.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/mockstack/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:520\u001b[39m, in \u001b[36mBaseChatModel.stream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m     err = \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo generation chunks were returned\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    519\u001b[39m     run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    522\u001b[39m run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n",
      "\u001b[31mValueError\u001b[39m: No generation chunks were returned"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"http://localhost:8000/ollama/openai/v1\",\n",
    "    api_key=\"SOME_STRING_THAT_DOES_NOT_MATTER\",\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"Write me a song about sparkling water.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
